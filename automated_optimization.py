# -*- coding: utf-8 -*-
"""automated optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1stb8hoow-tBSLO4Ux7UGxmo1dXc4KtC0
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50, VGG16, EfficientNetB0
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow_datasets as tfds
from tensorflow.keras.datasets import mnist, cifar10, fashion_mnist, cifar100

tf.config.optimizer.set_jit(True)

def load_mnist_data():
    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()
    train_images, test_images = train_images / 255.0, test_images / 255.0
    train_images = np.expand_dims(train_images, axis=-1)
    test_images = np.expand_dims(test_images, axis=-1)
    return (train_images, train_labels), (test_images, test_labels)

def load_cifar10_data():
    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()
    train_images, test_images = train_images / 255.0, test_images / 255.0
    return (train_images, train_labels), (test_images, test_labels)

def load_fashion_mnist_data():
    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
    train_images, test_images = train_images / 255.0, test_images / 255.0
    train_images = tf.image.resize(train_images[..., tf.newaxis], (32, 32)).numpy()
    test_images = tf.image.resize(test_images[..., tf.newaxis], (32, 32)).numpy()

    return (train_images, train_labels), (test_images, test_labels)

def load_cifar100_data():
    (train_images, train_labels), (test_images, test_labels) = cifar100.load_data()
    train_images, test_images = train_images / 255.0, test_images / 255.0
    return (train_images, train_labels), (test_images, test_labels)

def load_svhn_data():
    (train_ds, test_ds), ds_info = tfds.load('svhn_cropped', split=['train', 'test'], as_supervised=True, with_info=True)

    def preprocess_image(image, label):
        image = tf.cast(image, tf.float32) / 255.0
        return image, label

    train_ds = train_ds.map(preprocess_image).batch(32)
    test_ds = test_ds.map(preprocess_image).batch(32)

    train_images, train_labels = [], []
    for img, lbl in train_ds:
        train_images.append(img.numpy())
        train_labels.append(lbl.numpy())

    test_images, test_labels = [], []
    for img, lbl in test_ds:
        test_images.append(img.numpy())
        test_labels.append(lbl.numpy())

    return (np.concatenate(train_images, axis=0), np.concatenate(train_labels, axis=0)), (np.concatenate(test_images, axis=0), np.concatenate(test_labels, axis=0))

def cnn_model(input_shape=(28, 28, 1), num_classes=10):
    model = models.Sequential([
        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def resnet_model(input_shape=(32, 32, 3), num_classes=2):
    base_model = tf.keras.applications.ResNet50(weights=None, include_top=False, input_shape=input_shape)
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(128, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def vgg16_model(input_shape=(28, 28, 1), num_classes=2):
    base_model = tf.keras.applications.VGG16(weights=None, include_top=False, input_shape=input_shape)
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(128, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def efficientnet_model(input_shape=(32, 32, 3), num_classes=2):
    base_model = EfficientNetB0(weights=None, include_top=False, input_shape=input_shape)
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(128, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def train_model(dataset_name='mnist'):
    if dataset_name == 'mnist':
        (train_images, train_labels), (test_images, test_labels) = load_mnist_data()
        num_classes = 10
        input_shape = (28, 28, 1)
        model = cnn_model(input_shape=input_shape, num_classes=num_classes)
    elif dataset_name == 'cifar10':
        (train_images, train_labels), (test_images, test_labels) = load_cifar10_data()
        num_classes = 10
        input_shape = (32, 32, 3)
        model = resnet_model(input_shape=input_shape, num_classes=num_classes)
    elif dataset_name == 'fashion_mnist':
        (train_images, train_labels), (test_images, test_labels) = load_fashion_mnist_data()
        num_classes = 10
        input_shape = (28, 28, 1)
        model = vgg16_model(input_shape=input_shape, num_classes=num_classes)
    elif dataset_name == 'cifar100':
        (train_images, train_labels), (test_images, test_labels) = load_cifar100_data()
        num_classes = 100
        input_shape = (32, 32, 3)
        model = resnet_model(input_shape=input_shape, num_classes=num_classes)
    elif dataset_name == 'svhn':
        (train_images, train_labels), (test_images, test_labels) = load_svhn_data()
        num_classes = 10
        input_shape = (32, 32, 3)
        model = efficientnet_model(input_shape=input_shape, num_classes=num_classes)
    else:
        raise ValueError("Unknown dataset")

    early_stopping = EarlyStopping(monitor='val_loss', patience=3)
    model.fit(train_images, train_labels, epochs=2, validation_data=(test_images, test_labels), callbacks=[early_stopping])

    test_loss, test_acc = model.evaluate(test_images, test_labels)
    print(f"Test Accuracy for {dataset_name}: {test_acc * 100:.2f}%")

datasets = ['mnist', 'cifar10', 'fashion_mnist', 'cifar100', 'svhn']
for dataset in datasets:
    print(f"Training on {dataset}...")
    train_model(dataset_name=dataset)

from sklearn.metrics import classification_report
import os
import matplotlib.pyplot as plt

def save_model(model, model_name):
    model_dir = "saved_models"
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    model_path = os.path.join(model_dir, f"{model_name}_model.h5")
    model.save(model_path)
    print(f"Model saved as {model_name}_model.h5")

def evaluate_model(model, test_images, test_labels, model_name):
    test_loss, test_acc = model.evaluate(test_images, test_labels)
    print(f"Test Accuracy for {model_name}: {test_acc * 100:.2f}%")
    print(f"Test Loss for {model_name}: {test_loss:.4f}")
    predictions = model.predict(test_images)
    predictions = np.argmax(predictions, axis=1)
    print(f"\nClassification Report for {model_name}:\n{classification_report(test_labels, predictions)}")

def plot_loss_accuracy(history, model_name):
    if history is None:
        print(f"No history data for {model_name}.")
        return

    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'{model_name} Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'{model_name} Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

def train_and_evaluate_model(dataset_name='mnist'):
    if dataset_name == 'mnist':
        (train_images, train_labels), (test_images, test_labels) = load_mnist_data()
        num_classes = 10
        input_shape = (28, 28, 1)
    elif dataset_name == 'cifar10':
        (train_images, train_labels), (test_images, test_labels) = load_cifar10_data()
        num_classes = 10
        input_shape = (32, 32, 3)
    elif dataset_name == 'fashion_mnist':
        (train_images, train_labels), (test_images, test_labels) = load_fashion_mnist_data()
        num_classes = 10
        input_shape = (28, 28, 1)
    elif dataset_name == 'cifar100':
        (train_images, train_labels), (test_images, test_labels) = load_cifar100_data()
        num_classes = 100
        input_shape = (32, 32, 3)
    elif dataset_name == 'svhn':
        (train_images, train_labels), (test_images, test_labels) = load_svhn_data()
        num_classes = 10
        input_shape = (32, 32, 3)
    else:
        raise ValueError("Unknown dataset")

    model = cnn_model(input_shape=input_shape, num_classes=num_classes)
    history = model.fit(train_images, train_labels, epochs=2, validation_data=(test_images, test_labels))
    save_model(model, dataset_name)
    evaluate_model(model, test_images, test_labels, dataset_name)
    plot_loss_accuracy(history, dataset_name)

datasets = ['mnist', 'cifar10', 'fashion_mnist', 'cifar100', 'svhn']
for dataset in datasets:
    print(f"\nTraining and evaluating model on {dataset}...")
    train_and_evaluate_model(dataset_name=dataset)

!apt-get remove --purge libnvinfer* libnvparsers* libcudnn*
!pip uninstall -y nvidia-tensorrt tensorrt

!apt-get update
!apt-get install -y --no-install-recommends \
    cuda-libraries-dev-11-8

!pip install --force-reinstall nvidia-tensorrt==8.6.1.6 \
    libnvinfer8==8.6.1.6-1+cuda11.8 \
    libnvinfer-plugin8==8.6.1.6-1+cuda11.8 \
    libnvinfer-dev==8.6.1.6-1+cuda11.8

import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import torch.onnx
import onnx
import tensorflow as tf
import numpy as np
import time

def tensorrt_infer(onnx_model_path, input_data):
    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, TRT_LOGGER)

    with open(onnx_model_path, 'rb') as model:
        if not parser.parse(model.read()):
            print('ERROR: Failed to parse the ONNX file.')
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            return None

    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30
    engine = builder.build_engine(network, config)
    context = engine.create_execution_context()


    input_shape = input_data.shape
    output_shape = (input_shape[0], 10)
    d_input = cuda.mem_alloc(input_data.nbytes)
    d_output = cuda.mem_alloc(output_shape[0] * output_shape[1] * 4)

    bindings = [int(d_input), int(d_output)]
    stream = cuda.Stream()
    cuda.memcpy_htod_async(d_input, input_data, stream)
    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)
    output_data = np.empty(output_shape, dtype=np.float32)
    cuda.memcpy_dtoh_async(output_data, d_output, stream)
    stream.synchronize()

    return output_data

def evaluate_with_tensorrt(dataset_name='mnist'):
    if dataset_name == 'mnist':
        (_, _), (test_images, test_labels) = load_mnist_data()
    start_time = time.time()
    predictions = tensorrt_infer("model.onnx", test_images)
    tensorrt_inference_time = time.time() - start_time

    print(f"Optimized Inference Time (TensorRT) for {dataset_name}: {tensorrt_inference_time:.5f} seconds")
evaluate_with_tensorrt(dataset_name='mnist')